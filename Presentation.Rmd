---
title: "Clustering GAM-Smoothed NFL Elo Ratings"
subtitle: "Something about Walking, Chalking, and Talking"
author: "Paul Harmon"
date: "Spring 2018"
output:
  beamer_presentation:
    theme: "CambridgeUS"
    colortheme: "crane"
    fonttheme: "serif"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(readr); library(mgcv);library(pander)
x <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/nfl-elo-game/master/data/nfl_games.csv")
x_new <- dplyr::filter(x, season > 2001) #filters out anything before 2002, when Texans joined the league
#mutate some new columns
x_1 <- x_new[,c("date","season","team1","elo1","playoff")]
x_2 <- x_new[,c("date","season","team2","elo2","playoff")]
names(x_1) <- names(x_2) <- c("date","season","team","elo","playoff")
x_full <- rbind(x_1,x_2)

#remove playoff games
x_np <- arrange(dplyr::filter(x_full, playoff == 0), date)
x_np$teamyear <- interaction(x_np$season,x_np$team)

#some commentary: it looks like these are, in a certain sense, marginal models for the average
#elo of the team during a given year. 
#Try to create a "long" dataset
#Structure - Observations of every game for each year for each team as rows
x_sort <- arrange(x_np, teamyear) #sort so I can add a game indicator by team/game
x_sort$GAME <- rep(c(1:16),32*15) #adds a game indicator for each variable

write.csv(x_sort,"elo.long.csv")
#that game variable will become a column
library(tidyr)
x_wide <- x_sort[,c(2,3,4,7)]%>% spread(key = GAME, value = elo) #check it out I put in a pipe operator!
#we have a wide dataset, can i fit gam to each season

game <- 1:16
x <- gam(unlist(x_wide[1,-c(1,2)])~s(game, k=4), bs = 'cs')
gam_Wide <- function(vector, gam.type = 'cs', max.df = 16){
  #create game variable
  game <- 1:16
  #takes a wide dataset (remove name and year first)
  x <- gam(unlist(vector) ~ s(game, k = max.df))
  
return(x)}

gam_list <- apply(x_wide[,-c(1,2)],1,gam_Wide) #returns a list of the stuff you need
names(gam_list) <- interaction(x_wide$season,x_wide$team)
x_wide$teamyear <- interaction(x_wide$season,x_wide$team)
#let's make a data frame of predicted gams
pred_mat <- matrix(0, nrow = length(gam_list), ncol = 16)
for(j in 1:length(gam_list)){
pred_mat[j,] <- predict(gam_list[[j]])  
}
rownames(pred_mat) <- names(gam_list)

#create dissimilarity matrix via R-package (although below might be interesting as well)
#uses L2 norm (Euclidean distances) but I'd be curious about Mahalonobis distances as well
library(fda.usc)
gam_distances <- metric.lp(pred_mat)

#let's go ahead and try some hierarchical clustering
library(mclust)
#is there a way to do medoid-based clustering?
library(cluster)
pam1 <- pam(as.dist(gam_distances), metric = "euclidean", k = 4)

#tables of clusters


```

##Introduction: What's the Big Deal?

My goal was to analyze trends in NFL team wining/losing behavior over time. The NFL is supposed to have __parity__, meaning that good teams eventually become bad teams and vice versa. But that isn't always the case in the short term - say, 20 year periods. The Bills haven't made the playoffs in 18 years. The Patriots have missed the playoffs just three times in that span.

The goal started as simply trying to cluster teams based on their behavior during the season, as ranked by their Elo rating. (We'll get to that...). However, several other questions quickly arose:

+ Do you need to know about all 16 games to classify a team's behavior during the season? Or do teams "reveal themselves" before the season is totally over?
+ How sensitive are clusters to smoothing bases and the clustering algorithm? (Hierarchical vs. Medoid Based)
+ How many clusters should there be? I think 4, but that's a totally subjective starting point. 



##What Are Elo Ratings?
Elo ratings are used by FiveThirtyEight to predict the outcome of head-to-head matchups between NFL teams. They were originally developed for prediction of chess matches but have been extended to sports analytics.

+ A way to rank teams and predict outcomes 
+ Teams with higher Elo Ratings should beat teams with lower Elo Ratings
+ Can be interpreted as point spreads for matchups (divide difference by 25 to get spread)
+ Average Elo is 1500 for an NFL team, and they are calculated prior to every game

##Elo Ratings: In Depth

In general, an Elo rating can be calulated as a function of three things: a __Previous Elo Score__, a __Per-Game Weighting Factor__, and a __Margin of Victory Factor__.  The cool feature of Elo ratings is that they can reflect the increased effect of a single game during a 16-game NFL season as compared to an 82-game NBA season, or 144-game MLB season. 

__A General Elo Formula:__
$$ R_t = R_{t-1}+ K*V*M_{ij}(S_{ij}-\mu_{ij}) $$

##Calculating Elo Ratings For NFL vs. Other Sports: The K Factor

In the NFL, there are only 16 games. Compare that to the NBA with 82 games or the MLB with 162 games. Because of this, each game counts more towards a team's Elo rating. Whereas an embarassing loss might hurt both an NBA and NFL team, it will cost the NFL team more Elo points. 

According to FiveThirtyEight, the optimal K value in the NFL is 20. K was initially set to 10 when ELO was developed for chess. 

##For Dan
Sometimes, it's worthwhile to stop talking about math and just appreciate how much it hurts to be a Patriots fan...
![Tom Brady, after a loss](tbb.jpg)



##Why Do We Care About Classifying Elo Ratings?
Team decision makers may want to develop a sense for the type of team that they have, given that not all teams realistically have a chance at  winning a championship in every year. Sorry, Broncos and Browns fans...

+ We want to make __comparisons__ between different teams in different seasons. 
+ Allows for assessment of team __parity__ over time. 

__Goal__: To __smooth__ season-long measurements of Elo ratings for each NFL team and __cluster__ team-year Elos into 4 groups.  Research by Hitchcock, Booth, and yes, _that_ George Casella (2007) indicates that smoothing improves cluster fits.

## Functional Data Analysis

__Functional Data__: Some data, especially over time, can be be expressed as a *function* of some explanatory variable. If data represent measurements of a continuous trend, we try to estimate it. 
![Functional Data](figure/funky.png)


##The Data
We have __32 teams__ with 16 games per season over 15 seasons, leading to 480 curves that need to be smoothed with GAMs. Each team/year combination is considered independent.

```{r plot_elo, eval = TRUE, fig.align = 'center', fig.width = 7, fig.height = 3}
ggplot(filter(x_sort, teamyear %in% c('2012.DEN','2007.NE','2008.DET','2016.NE'))) + 
  geom_line(aes(x = GAME, y = elo, group = teamyear,col = teamyear),lwd = 2) +
  theme_bw() + xlab("Game") + ylab("Elo Rating") + ggtitle("Four Teams' Elo Ratings") + 
  scale_color_manual(values=c("orange", "lightblue", "red","blue4"))
```


##Tools for Smoothing: GAMs
One could use a linear regression model to smooth noisy functions in the data, but this may over-linearize the relationship. Similarly, fitting a polynomial may overfit the data or require too many parameters to estimate. A more parsimonious solution is to fit a GAM, which estimates smooth functions of the data. 

The next slide compares a GAM smooth to a linear and 5th order polynomial. You could go so far as to fit a 15th order polynomial here since we have 16 data points.  

##GAM Comparison

```{r, fig.align = 'center', fig.width = 7, fig.height = 5}
x <- predict(gam_list[[472]])
model <- lm(x ~ game)
mod_poly <- lm(x ~ poly(game, 5))

plot(1:16,predict(gam_list[[472]]), type = "l", xlab = "Game", ylab = "Elo", lwd = 2, col = "red3", ylim = c(1370, 1550))
points(1:16, x_wide[472,-c(1,2,19)], col = "green3", pch = 18, cex = 1.2)
lines(1:16,predict(model), lwd = 2, lty = 2, col = "gray30")
lines(1:16, predict(mod_poly), lwd = 2, col = "purple")
title("New York Jets 2016 Data vs. Smooth GAM")
```


##Things to Think About

GAMS rely on a choice of basis function:

+ **Thin Plate **: Doesn't require knots - minimizes distance between data and predicted values with 'wiggliness' penalty term
+ **Cubic Regression **: Fits cubic spline to the data on a set of partitions to the data 
+ Other options.... this could probably be a talk in and of itself

**Effective Degrees of Freedom**: You can allow the GAM to estimate a maximum wiggliness. More EDF results in more wiggle and fewer EDF looks more linear.   

##GAM-Smoothed Estimates

+ Elo Ratings can be a bit noisy! It might be better to estimate a mean trend. 
+ I used GAMS to estimate smooth versions of these noisy trends in Elo.

__A Very Complicated Model: __ 
$$ \hat{Elo}_{ij} = s_{i}(game_{ij})$$
_Notationally, i refers to team/year combination (1..480) and j refers to game within season (1...16)._  

+ Technically, there are 480 smoothed models fit here
+ They may involve differing EDFs for each model depending on how wiggly things need to be
+ Majority of the EDF were below 5.72, with 90 percent having small p-values

```{r, include = FALSE}
pval_get <- function(gam.object){
  smooth_pval <- summary(gam.object)$s.pv
  
 return(s.pval = smooth_pval)}

edf_get <- function(gam.object){
  edf <- summary(gam.object)$edf
  return(edf = edf)}

edf_values <- sapply(gam_list, edf_get)
summary(edf_values)

p_values <- sapply(gam_list, pval_get)
summary(p_values)
```

## ##Functional Data Clustering
We can then take these curves and calculate distances between each of the GAM smoothers. We can generate a 4-cluster solution based on a technique called __Partitioning Around Medoids__ (PAM). 
The Idea: 

+ Determine 4 "medoid" GAM fits
+ For each GAM, figure out which medoid is closest - that's the cluster that each observation goes in



##Functional Data clustering: 4-Cluster Solution
__Contenders__: 2010 Baltimore Ravens, __Status Quo__: 2010 Miami Dolphins, __Pretenders__: 2009 Buffalo Bills, __Loveable Losers__: 2008 Oakland Raiders

```{r,warning = FALSE, message = FALSE, eval = TRUE, fig.align = 'center', fig.width = 6, fig.height = 2.5}

ggplot(filter(x_sort, teamyear %in% pam1$medoids)) + 
  geom_line(aes(x = GAME, y = elo, group = teamyear,col = teamyear),lwd = 1.2) +
  theme_bw() + xlab("Game") + ylab("Elo Rating") + ggtitle("Medoid Elo Ratings") + 
  scale_color_manual(values=c("orange", "lightblue", "red","blue4"))


```

##Some Interesting Team Results
```{r}
pander(table(x_wide$team,pam1$clustering)[c(4,10,21,23),])
```

##Long Term Elo Trends 
Why not model long-term trends? We could ignore the fact that the Elos are normalized and try to model the long-term trends if we wanted to. 
```{r, warning = FALSE, message = FALSE, fig.align = 'center', fig.height = 4}
plot(x_np$date[x_np$team == "DEN"],x_np$elo[x_np$team == "DEN"], type = "p", xlab = "Year", ylab = "ELO", pch = 20, col = "orange2")
title("Denver Broncos Elos since 2002")

library(mgcv)
den.dat <- x_np[x_np$team == "DEN",]
gam1 <- gam(elo ~ s(as.numeric(date), k = 16), data = den.dat) #tensor product
gam2 <- gam(elo ~ s(as.numeric(date), k = 16, bs = "cs"), data = den.dat) #cubicshrinkage
gam3 <- gam(elo ~ s(as.numeric(date), k = 16, bs = "ds"), data = den.dat)
gam4 <- gam(elo ~ s(as.numeric(date), k = 16, bs = "cr"), data = den.dat)

gamm1 <- gamm(elo ~ s(as.numeric(date), k = 16, bs = "cs"), random = , data = den.dat)

lines(den.dat$date, predict(gam1), col = "blue3", lwd = 2)
lines(den.dat$date, predict(gam2), col = "red", lwd = 2, lty = 3)
lines(den.dat$date, predict(gam3), col = "pink", lwd = 2, lty = 4)
lines(den.dat$date, predict(gam4), col = "purple", lwd = 2, lty = 2)

legend('bottomright', legend = c("TP","CS","DS","CR"), fill = c("blue3","red","pink","purple"))
```


## Do Teams Reveal Themselves in 10-games?

## Do basis functions really matter? 

## Does smoothing really matter? 

## How much do clustering algorithms matter? 




##Selected References

Glickman, Mark E. and Jones, Albyn (1999)."Rating the Chess Rating System". _Chance_. pp.21-28.

Silver,Nate (2014). "Introducing NFL Elo Ratings" _https://fivethirtyeight.com/features/introducing-nfl-elo-ratings_

Hitchcock, David B., Booth, James G., and Casella, George. (2007). "The Effect of Pre-Smoothing Functional Data on Cluster Analysis." _The Journal of Statistical Computation and Simulation_. 
pp. 1043-1055.

Wood, Simon.(2017). _Generalized Additive Models: An Introduction with R_. Ed 2.Chapman and Hall/CRC. 


##Questions

![The dude himself... Trevor Siemian](figure/siemian.jpg)




